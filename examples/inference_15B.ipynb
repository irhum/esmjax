{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks sets up an ESM-2 15B model, with weights loaded in from PyTorch, for inference on a TPU v2-8/v3-8, taking full advantage of model parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only necessary in non-Poetry envs, remove once pip-installable.\n",
    "# import sys\n",
    "# sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "\n",
    "from flax.core import frozen_dict\n",
    "import jax\n",
    "\n",
    "# esmjax imports\n",
    "from esmjax import io, tokenizer as esm_tokenizer\n",
    "from esmjax.modules import modules\n",
    "\n",
    "# Imports specifically for multi-device sharding\n",
    "from esmjax.modules import partitioning\n",
    "from flax.linen import partitioning as nn_partitioning\n",
    "from jax.experimental import maps, PartitionSpec as P, pjit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load model\n",
    "\n",
    "First, we load in the model and its converted weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"esm2_t48_15B_UR50D\"\n",
    "# Load in the original PyTorch state; will download if first time.\n",
    "state = io.get_torch_state(MODEL_NAME)\n",
    "\n",
    "esm, params_axes = modules.get_esm2_model(state[\"cfg\"])\n",
    "esm_params = io.convert_encoder(state[\"model\"], state[\"cfg\"])\n",
    "esm_params = frozen_dict.FrozenDict({\"params\": esm_params})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `AxisMetadata`, pre-conversion. As we see, the metadata exists only for the params that are to be sharded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    fc1: {\n",
       "        kernel_axes: AxisMetadata(names=('embed_kernel', 'hidden')),\n",
       "    },\n",
       "    fc2: {\n",
       "        kernel_axes: AxisMetadata(names=('hidden', 'embed_kernel')),\n",
       "    },\n",
       "    self_attn: {\n",
       "        k_proj: {\n",
       "            kernel_axes: AxisMetadata(names=('embed_kernel', 'heads', None)),\n",
       "        },\n",
       "        out_proj: {\n",
       "            kernel_axes: AxisMetadata(names=('heads', None, 'embed_kernel')),\n",
       "        },\n",
       "        q_proj: {\n",
       "            kernel_axes: AxisMetadata(names=('embed_kernel', 'heads', None)),\n",
       "        },\n",
       "        v_proj: {\n",
       "            kernel_axes: AxisMetadata(names=('embed_kernel', 'heads', None)),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_axes[\"3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shard the loaded params onto the TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
       " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
       " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
       " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
       " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking we have 8 devices\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert `params_axes` (which only has sharding specs for params\n",
    "# that will be sharded) to `esm_params` (which has a spec for ALL\n",
    "# params, defaulting to None for params that are fully replicated.)\n",
    "esm_axes = partitioning.get_params_axes(esm_params, params_axes, rules=partitioning.DEFAULT_TPU_RULES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check the sharding spec for layer 3, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    fc1: {\n",
       "        kernel: PartitionSpec('X', 'Y'),\n",
       "        bias: None,\n",
       "    },\n",
       "    fc2: {\n",
       "        kernel: PartitionSpec('Y', 'X'),\n",
       "        bias: None,\n",
       "    },\n",
       "    self_attn_layer_norm: {\n",
       "        scale: None,\n",
       "        bias: None,\n",
       "    },\n",
       "    final_layer_norm: {\n",
       "        scale: None,\n",
       "        bias: None,\n",
       "    },\n",
       "    self_attn: {\n",
       "        k_proj: {\n",
       "            kernel: PartitionSpec('X', 'Y', None),\n",
       "            bias: None,\n",
       "        },\n",
       "        q_proj: {\n",
       "            kernel: PartitionSpec('X', 'Y', None),\n",
       "            bias: None,\n",
       "        },\n",
       "        v_proj: {\n",
       "            kernel: PartitionSpec('X', 'Y', None),\n",
       "            bias: None,\n",
       "        },\n",
       "        out_proj: {\n",
       "            kernel: PartitionSpec('Y', None, 'X'),\n",
       "            bias: None,\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esm_axes[\"params\"][\"3\"]  # looks right! Note we're only sharding the large kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D TPU mesh\n",
    "mesh_shape = (2, 4)  # X=2, Y=4, 8 TPUs total\n",
    "devices = np.asarray(jax.devices()).reshape(*mesh_shape)\n",
    "mesh = maps.Mesh(devices, (\"X\", \"Y\"))\n",
    "\n",
    "# Create fn for inference.\n",
    "preshard_fn = pjit.pjit(\n",
    "    lambda x: x,  # this function does nothing\n",
    "    in_axis_resources=(esm_axes,),  # but this spec \"pre-shards\" the params\n",
    "    out_axis_resources=esm_axes,\n",
    ")\n",
    "\n",
    "# There's two contexts: one for the mesh, the other specifying the translation\n",
    "# rules for named sharding axis -> TPU mesh logical axis\n",
    "with maps.Mesh(mesh.devices, mesh.axis_names), nn_partitioning.axis_rules(\n",
    "    partitioning.DEFAULT_TPU_RULES\n",
    "):\n",
    "    esm_sharded_params = preshard_fn(esm_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the mesh object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mesh(array([[0, 1, 2, 3],\n",
       "       [4, 5, 6, 7]]), ('X', 'Y'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access sharding specs down to individual params, if we'd like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShardingSpec((Chunked(2), Chunked(4)), (ShardedAxis(axis=0), ShardedAxis(axis=1)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esm_sharded_params[\"params\"][\"3\"][\"fc1\"][\"kernel\"].sharding_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShardingSpec((Chunked(4), Chunked(2)), (ShardedAxis(axis=1), ShardedAxis(axis=0)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esm_sharded_params[\"params\"][\"3\"][\"fc2\"][\"kernel\"].sharding_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the exact indices ranges that exist on each TPU. For example, here we see each TPU has a unique 2560x5120 sized \"slice\" of the 5120x20480 weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((slice(0, 2560, None), slice(0, 5120, None)),\n",
       " (slice(0, 2560, None), slice(5120, 10240, None)),\n",
       " (slice(0, 2560, None), slice(10240, 15360, None)),\n",
       " (slice(0, 2560, None), slice(15360, 20480, None)),\n",
       " (slice(2560, 5120, None), slice(0, 5120, None)),\n",
       " (slice(2560, 5120, None), slice(5120, 10240, None)),\n",
       " (slice(2560, 5120, None), slice(10240, 15360, None)),\n",
       " (slice(2560, 5120, None), slice(15360, 20480, None)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esm_sharded_params[\"params\"][\"3\"][\"fc1\"][\"kernel\"].indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Tokenize input protein\n",
    "\n",
    "For this example, we use the sequences for [p53](https://en.wikipedia.org/wiki/P53) (one of the most extensively studied proteins in cancer biology) and insulin. The sequence for the human orthologs are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p53_seq = \"MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGP\\\n",
    "    DEAPRMPEAAPPVAPAPAAPTPAAPAPAPSWPLSSSVPSQKTYQGSYGFRLGFLHSGTAK\\\n",
    "    SVTCTYSPALNKMFCQLAKTCPVQLWVDSTPPPGTRVRAMAIYKQSQHMTEVVRRCPHHE\\\n",
    "    RCSDSDGLAPPQHLIRVEGNLRVEYLDDRNTFRHSVVVPYEPPEVGSDCTTIHYNYMCNS\\\n",
    "    SCMGGMNRRPILTIITLEDSSGNLLGRNSFEVRVCACPGRDRRTEEENLRKKGEPHHELP\\\n",
    "    PGSTKRALPNNTSSSPQPKKKPLDGEYFTLQIRGRERFEMFRELNEALELKDAQAGKEPG\\\n",
    "    GSRAHSSHLKSKKGQSTSRHKKLMFKTEGPDSD\"\n",
    "\n",
    "insulin_seq = \"MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAED\\\n",
    "    LQVGQVELGGGPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use our tokenizer to convert these sequence of letters into sequence of integers, with appropriate padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = esm_tokenizer.protein_tokenizer(pad_to_multiple_of=128)\n",
    "tokens = [x.ids for x in tokenizer.encode_batch([p53_seq, insulin_seq])]\n",
    "batch = np.array(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and last tokens are 0 and 2, `<cls>` and `<eos>`. Note that the first actual amino acid in both sequences is 20, which is methionine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 20,  9, ...,  1,  1,  1],\n",
       "       [ 0, 20,  5, ...,  1,  1,  1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get embeddings\n",
    "\n",
    "We then create a `pjit`'ted function for inference, and call it just like the parameter sharding function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fn for inference.\n",
    "apply_fn = pjit.pjit(\n",
    "    esm.apply,\n",
    "    in_axis_resources=(esm_axes, P(\"X\", None)),\n",
    "    out_axis_resources=P(\"X\", None, \"Y\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the first call takes a *while*, about 50s on a TPUv2-8\n",
    "with maps.Mesh(mesh.devices, mesh.axis_names), nn_partitioning.axis_rules(\n",
    "    partitioning.DEFAULT_TPU_RULES\n",
    "):\n",
    "    embeds = apply_fn(esm_sharded_params, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeds is a 2x512x5120 tensor, corresponding to batch x seq x features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 512, 5120)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see its sharding pattern too; the batch axis is sharded across the X mesh axis, and the embedding axis is sharded over the Y mesh axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ShardingSpec((Chunked(2), NoSharding(), Chunked(4)), (ShardedAxis(axis=0), ShardedAxis(axis=1))),\n",
       " ((slice(0, 1, None), slice(None, None, None), slice(0, 1280, None)),\n",
       "  (slice(0, 1, None), slice(None, None, None), slice(1280, 2560, None)),\n",
       "  (slice(0, 1, None), slice(None, None, None), slice(2560, 3840, None)),\n",
       "  (slice(0, 1, None), slice(None, None, None), slice(3840, 5120, None)),\n",
       "  (slice(1, 2, None), slice(None, None, None), slice(0, 1280, None)),\n",
       "  (slice(1, 2, None), slice(None, None, None), slice(1280, 2560, None)),\n",
       "  (slice(1, 2, None), slice(None, None, None), slice(2560, 3840, None)),\n",
       "  (slice(1, 2, None), slice(None, None, None), slice(3840, 5120, None))))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.sharding_spec, embeds.indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "723e9f8f337666420e6311b8427c3df11ea0bb9887911b176cc3121128351b10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
